{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462f78d-c789-4162-9e0f-a2242a86582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from google.cloud import storage\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name):\n",
    "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "    # Note: The call returns a response only when the iterator is consumed.\n",
    "    # for blob in blobs:\n",
    "    #     print(blob.name)\n",
    "    return [blob.name for blob in blobs]\n",
    "\n",
    "\n",
    "def get_file_list(bucket_name):\n",
    "    cmd  = f\"gsutil ls gs://{bucket_name}\"\n",
    "    #if os.system(cmd) == 0:\n",
    "    status,files = subprocess.getstatusoutput(cmd)\n",
    "    if status == 0:\n",
    "        return files.split(\"\\n\")\n",
    "    else:\n",
    "        raise Exception(\"no file was found\")\n",
    "\n",
    "def get_tfrecord_files(bucket_file_list,suffix='tfrecord'):\n",
    "    return [ f for f in bucket_file_list if f.endswith(suffix) or f.endswith('gz')]\n",
    "\n",
    "def get_json_file(gsfiles,file_prefix=\"mixer.json\"):\n",
    "    json_file_list = [f for f in gsfiles if file_prefix in f]\n",
    "    if json_file_list:\n",
    "        json_file = json_file_list[0] #str(path/(file_prefix+'mixer.json'))\n",
    "        cmd = f\"gsutil cat {json_file}\"\n",
    "        status,text = subprocess.getstatusoutput(cmd)\n",
    "        if status == 0:\n",
    "            mixer = json.loads(text)\n",
    "            return mixer\n",
    "        else:\n",
    "            raise Exception(\"no json file was found\")\n",
    "\n",
    "# Parsing function.\n",
    "def parse_image(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_features_dict)\n",
    "\n",
    "\n",
    "def select_tiles_on_classRatio(ds_np_gen , img_size= 256*256, class_ratio=0.5):\n",
    "    thr = class_ratio * img_size\n",
    "    for img_dic in  ds_np_gen:\n",
    "        img = img_dic['cwf']\n",
    "        if np.count_nonzero(img) >= thr:\n",
    "            yield img_dic\n",
    "\n",
    "def np_to_tfr(ds_gen, file_name=\"./test_tfRecord.gz\"):\n",
    "    with tf.io.TFRecordWriter(file_name,options=tf.io.TFRecordOptions(\n",
    "    compression_type='GZIP')) as writer:\n",
    "        for img_dic in ds_gen:\n",
    "            \n",
    "            feature = {}\n",
    "            for k, v in img_dic.items():\n",
    "                if k == 'cwf':\n",
    "                    feature[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v.flatten()))\n",
    "                else:\n",
    "                    feature[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v.flatten()))                \n",
    "            \n",
    "            # Construct the Example proto object\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "            # Serialize the example to a string\n",
    "            serialized = example.SerializeToString()\n",
    "\n",
    "            # write the serialized objec to the disk\n",
    "            writer.write(serialized)\n",
    "\n",
    "def write_file_to_gs(file_name,bucket_name,blob_name):\n",
    "    JSON_FILE_NAME = '/home/layla/service_account.json'\n",
    "    client = storage.Client.from_service_account_json(JSON_FILE_NAME)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(file_name)\n",
    "\n",
    "def delete_file_from_disk(file_name):\n",
    "    cmd = f\"rm {file_name}\"\n",
    "    status, _ = subprocess.getstatusoutput(cmd)\n",
    "    return status\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed13adb-6510-4a33-be15-bd345788c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"image_tiles_us_florida\"\n",
    "gsfiles = get_file_list(bucket_name)#list_blobs(bucket_name) \n",
    "test_files_list = get_tfrecord_files(gsfiles)\n",
    "mixer = get_json_file(gsfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant info from the JSON mixer file.\n",
    "patch_width = mixer['patchDimensions'][0]\n",
    "patch_height = mixer['patchDimensions'][1]\n",
    "patches = mixer['totalPatches']\n",
    "patch_dimensions_flat = [patch_width, patch_height]\n",
    "patch_size = patch_width * patch_height\n",
    "\n",
    "\n",
    "#bands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', 'mndwi', 'ndwi']\n",
    "bands = ['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7','ST_B10','NDVI','NDWI','SR','EVI','OSAVI',\n",
    "         'SR_B2_1','SR_B3_1','SR_B4_1','SR_B5_1','SR_B6_1','SR_B7_1','ST_B10_1','NDVI_1','NDWI_1','SR_1','EVI_1','OSAVI_1']\n",
    "image_columns = [\n",
    "  tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32)\n",
    "  for k in bands]\n",
    "\n",
    "bands += ['cwf']\n",
    "image_columns += [tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.int64)]\n",
    "\n",
    "# Parsing dictionary.\n",
    "image_features_dict = dict(zip(bands, image_columns))\n",
    "#image_features_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d97eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2a0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tiles in processed tfrecord files\n",
    "for file in test_files_list[:7]:\n",
    "    image_dataset = tf.data.TFRecordDataset(file, compression_type='GZIP')\n",
    "    ds = image_dataset.map(parse_image, num_parallel_calls=5)\n",
    "    ds_np_gen = ds.as_numpy_iterator()\n",
    "    print(len(list(ds_np_gen)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_list[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eadd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tiles in original tfrecord files\n",
    "for file in test_files_list[7:]:\n",
    "    image_dataset = tf.data.TFRecordDataset(file, compression_type='GZIP')\n",
    "    ds = image_dataset.map(parse_image, num_parallel_calls=5)\n",
    "    ds_np_gen = ds.as_numpy_iterator()\n",
    "    print(len(list(ds_np_gen)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5ffd45",
   "metadata": {},
   "source": [
    "#### plot some of tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = tf.data.TFRecordDataset(test_files_list[0], compression_type='GZIP')\n",
    "# bad, parse_image, only takes one input, feature_dic must be run in notebook??\n",
    "ds = image_dataset.map(parse_image, num_parallel_calls=5)\n",
    "ds_np_gen = ds.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c41bd-c691-4545-a866-dec7cb11f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_img_dic = next(ds_np_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249560b-8bc5-4f18-b6d0-ba07b0667506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patch(ax, arr, cmap=None):\n",
    "    ax.imshow(arr, cmap=cmap)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15,11))\n",
    "\n",
    "rgb = np.concatenate((gs_img_dic['SR_B4'][..., None], gs_img_dic['SR_B3'][..., None], gs_img_dic['SR_B2'][..., None]), axis=2)\n",
    "plot_patch(ax[0], rgb*3)\n",
    "plot_patch(ax[1], gs_img_dic['cwf'], 'Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8d533-637b-40ea-a8e8-2d43a37ecb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
