{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462f78d-c789-4162-9e0f-a2242a86582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from google.cloud import storage\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b28af1-9751-48bb-bae8-222838ec77bb",
   "metadata": {},
   "source": [
    "### Read from google storage bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_blobs(bucket_name):\n",
    "    \"\"\"Lists all the blobs in the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Note: Client.list_blobs requires at least package version 1.17.0.\n",
    "    blobs = storage_client.list_blobs(bucket_name)\n",
    "\n",
    "    # Note: The call returns a response only when the iterator is consumed.\n",
    "    # for blob in blobs:\n",
    "    #     print(blob.name)\n",
    "    return [blob.name for blob in blobs]\n",
    "\n",
    "\n",
    "def get_file_list(bucket_name):\n",
    "    cmd  = f\"gsutil ls gs://{bucket_name}\"\n",
    "    #if os.system(cmd) == 0:\n",
    "    status,files = subprocess.getstatusoutput(cmd)\n",
    "    if status == 0:\n",
    "        return files.split(\"\\n\")\n",
    "    else:\n",
    "        raise Exception(\"no file was found\")\n",
    "\n",
    "def get_tfrecord_files(bucket_file_list,suffix='tfrecord'):\n",
    "    return [ f for f in bucket_file_list if f.endswith(suffix) or f.endswith('gz')]\n",
    "\n",
    "def get_json_file(gsfiles,file_prefix=\"mixer.json\"):\n",
    "    json_file_list = [f for f in gsfiles if file_prefix in f]\n",
    "    if json_file_list:\n",
    "        json_file = json_file_list[0] #str(path/(file_prefix+'mixer.json'))\n",
    "        cmd = f\"gsutil cat {json_file}\"\n",
    "        status,text = subprocess.getstatusoutput(cmd)\n",
    "        if status == 0:\n",
    "            mixer = json.loads(text)\n",
    "            return mixer\n",
    "        else:\n",
    "            raise Exception(\"no json file was found\")\n",
    "\n",
    "# Parsing function.\n",
    "# TO DO: include img_feature_dict as input so I can call it\n",
    "# ds = image_dataset.map(lambda proto, features: tf.io.parse_single_example(proto, features),\n",
    "def parse_image(example_proto):\n",
    "    return tf.io.parse_single_example(example_proto, image_features_dict)\n",
    "\n",
    "\n",
    "def select_tiles_on_classRatio(ds_np_gen , img_size = 256*256 ,class_ratio=0.5):\n",
    "    thr = class_ratio * img_size\n",
    "    for img_dic in  ds_np_gen:\n",
    "        img = img_dic['cwf']\n",
    "        if np.count_nonzero(img) >= thr:\n",
    "            yield img_dic\n",
    "\n",
    "def np_to_tfr(ds_gen, file_name=\"./test_tfRecord.gz\"):\n",
    "    with tf.io.TFRecordWriter(file_name,options=tf.io.TFRecordOptions(\n",
    "    compression_type='GZIP')) as writer:\n",
    "        for img_dic in ds_gen:\n",
    "            \n",
    "            feature = {}\n",
    "            for k, v in img_dic.items():\n",
    "                if k == 'cwf':\n",
    "                    feature[k] = tf.train.Feature(int64_list=tf.train.Int64List(value=v.flatten()))\n",
    "                else:\n",
    "                    feature[k] = tf.train.Feature(float_list=tf.train.FloatList(value=v.flatten()))                \n",
    "            \n",
    "            # Construct the Example proto object\n",
    "            example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "            # Serialize the example to a string\n",
    "            serialized = example.SerializeToString()\n",
    "\n",
    "            # write the serialized objec to the disk\n",
    "            writer.write(serialized)\n",
    "\n",
    "def write_file_to_gs(file_name,bucket_name,blob_name):\n",
    "    JSON_FILE_NAME = '/home/layla/service_account.json'\n",
    "    client = storage.Client.from_service_account_json(JSON_FILE_NAME)\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(file_name)\n",
    "\n",
    "def delete_file_from_disk(file_name):\n",
    "    cmd = f\"rm {file_name}\"\n",
    "    status, _ = subprocess.getstatusoutput(cmd)\n",
    "    return status\n",
    "\n",
    "\n",
    "def select_and_export_tiles_to_gs(files_list, blob_prefix=\"test_tfRecord\"):\n",
    "    for i in range(len(files_list)):\n",
    "        blob_name = f\"{blob_prefix}{i+1}.gz\"\n",
    "        file_name = f\"./{blob_name}\"\n",
    "        print(\"processing tile file\", i)\n",
    "        image_dataset = tf.data.TFRecordDataset(files_list[i], compression_type='GZIP')\n",
    "        ds = image_dataset.map(parse_image, num_parallel_calls=5)\n",
    "                                          num_parallel_calls=5)\n",
    "\n",
    "        ds_np_gen = ds.as_numpy_iterator()\n",
    "        ds_gen = select_tiles_on_classRatio(ds_np_gen,0.3)\n",
    "        np_to_tfr(ds_gen,file_name=file_name)\n",
    "        write_file_to_gs(file_name,bucket_name,blob_name)\n",
    "        delete_file_from_disk(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed13adb-6510-4a33-be15-bd345788c8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"image_tiles_us_florida\"\n",
    "gsfiles = get_file_list(bucket_name)#list_blobs(bucket_name) \n",
    "files_list = get_tfrecord_files(gsfiles)\n",
    "mixer = get_json_file(gsfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28827dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant info from the JSON mixer file.\n",
    "patch_width = mixer['patchDimensions'][0]\n",
    "patch_height = mixer['patchDimensions'][1]\n",
    "patches = mixer['totalPatches']\n",
    "patch_dimensions_flat = [patch_width, patch_height]\n",
    "patch_size = patch_width * patch_height\n",
    "\n",
    "bands = ['SR_B2','SR_B3','SR_B4','SR_B5','SR_B6','SR_B7','ST_B10','NDVI','NDWI','SR','EVI','OSAVI',\n",
    "    'SR_B2_1','SR_B3_1','SR_B4_1','SR_B5_1','SR_B6_1','SR_B7_1','ST_B10_1','NDVI_1','NDWI_1','SR_1','EVI_1','OSAVI_1']\n",
    "\n",
    "image_columns = [tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32) for k in bands]\n",
    "\n",
    "bands += ['cwf']\n",
    "\n",
    "image_columns += [tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.int64)]\n",
    "\n",
    "# Parsing dictionary.\n",
    "image_features_dict = dict(zip(bands, image_columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d97eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_and_export_tiles_to_gs(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41363a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# files_list = files_list[:2]\n",
    "# for i in range(len(files_list)):\n",
    "#     blob_name = f\"test_tfRecord{i+1}.gz\"\n",
    "#     file_name = f\"./{blob_name}\"\n",
    "#     print(i)\n",
    "#     # Note that you can make one dataset from many files by specifying a list.\n",
    "#     image_dataset = tf.data.TFRecordDataset(files_list[i], compression_type='GZIP')\n",
    "#     ds = image_dataset.map(parse_image, num_parallel_calls=5)\n",
    "#     ds_np_gen = ds.as_numpy_iterator()\n",
    "#     ds_gen = select_tiles_on_classRatio(ds_np_gen,0.3)\n",
    "    \n",
    "#     np_to_tfr(ds_gen,file_name=file_name)\n",
    "#     write_file_to_gs(file_name,bucket_name,blob_name)\n",
    "#     delete_file_from_disk(file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8d533-637b-40ea-a8e8-2d43a37ecb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
